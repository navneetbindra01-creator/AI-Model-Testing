**An AI Model Test Strategy**

AI models are non-deterministic. Each type you prompt the model with a question/request, it will deliver a slightly different result. 

This makes standard comparison between actual and expected results a tricky endeavour. 

In this project, I explore the use of **cosine similarity**. This is a statistical method for comparing actual and expected result by measuring semantic similarity. 
I recommend further reading on the cosine similarity topic: https://www.ibm.com/think/topics/cosine-similarity

To further my dive into AI model validation, I followed up on a recent meeting with the Giskard team. They specialise in testing AI models and stand by a mission to help build AI safely. I was impressed. 
I decided to explore Giskard further and managed to use their suite of libraries in python to help detect **Hallucination** and other nuances such as **Stereotyping**. 

The AI model under test is xAI (Grok). But it can easily be substituted with your model of choice. 

The below is a sample test report generated by this test model. 

<img width="1883" height="842" alt="image" src="https://github.com/user-attachments/assets/d443ee25-aa05-481c-ba6c-bd509f92c4d4" />
